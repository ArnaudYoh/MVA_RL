{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJ_s4_nhhiyo"
   },
   "source": [
    "# Reinforcement Learning with Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTOWkpoCiLmL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.2.0 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: torchvision in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (0.3.0)\n",
      "Requirement already satisfied: pyvirtualdisplay in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: matplotlib in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (3.1.0)\n",
      "Requirement already satisfied: seaborn in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (1.17.3)\n",
      "Requirement already satisfied: pathlib in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: gym in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.1)\n",
      "Requirement already satisfied: six in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: EasyProcess in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from pyvirtualdisplay) (0.2.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.6.0.post20191030)\n",
      "Requirement already satisfied: future in /home/naunauyoh/anaconda3/lib/python3.7/site-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.2.0 torchvision pyvirtualdisplay matplotlib seaborn pandas numpy pathlib gym\n",
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
    "!cd mvarl_hands_on && git pull origin master > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCsyvh4NhjUc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from pprint import pprint\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# The following code is will be used to visualize the environments.\n",
    "\n",
    "def show_video(directory):\n",
    "    html = []\n",
    "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start();\n",
    "\n",
    "def make_seed(seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThnSNaSMhnXa"
   },
   "source": [
    "PyTorch is a Python package that provides two high-level features:\n",
    "- Tensor computation (like NumPy) with strong GPU acceleration\n",
    "- Deep neural networks built on a tape-based autograd system\n",
    "\n",
    "At a granular level, PyTorch is a library that consists of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "| ---- | --- |\n",
    "| [**torch**](https://pytorch.org/docs/stable/torch.html) | a Tensor library like NumPy, with strong GPU support |\n",
    "| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n",
    "| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n",
    "| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n",
    "| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n",
    "| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n",
    "\n",
    "\n",
    "**Tutorials on PyTorch:** https://pytorch.org/tutorials/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlZQl9TjhuWY"
   },
   "source": [
    "## OpenAI gym\n",
    "We will consider environments provided by OpenAI gym\n",
    "This library provides a large number of environments to test RL algorithm.\n",
    "\n",
    "We will focus on the **CartPole-v1** environment in this lab but we encourage you to also test your code on:\n",
    "* **Acrobot-v1**\n",
    "* **MountainCar-v0**\n",
    "\n",
    "| Env Info          \t| CartPole-v1 \t| Acrobot-v1                \t| MountainCar-v0 \t|\n",
    "|-------------------\t|-------------\t|---------------------------\t|----------------\t|\n",
    "| **Observation Space** \t| Box(4)      \t| Box(6)                    \t| Box(2)         \t|\n",
    "| **Action Space**      \t| Discrete(2) \t| Discrete(3)               \t| Discrete(3)    \t|\n",
    "| **Rewards**           \t| 1 per step  \t| -1 if not terminal else 0 \t| -1 per step    \t|\n",
    "\n",
    "A gym environment is loaded with the command `env = gym.make(env_id)`. Once the environment is created, you need to reset it with `observation = env.reset()` and then you can interact with it using the method step: `observation, reward, done, info = env.step(action)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wfHvW9Hh4H3"
   },
   "outputs": [],
   "source": [
    "# We load CartPole-v1\n",
    "env = gym.make('CartPole-v1')\n",
    "# We wrap it in order to save our experiment on a file.\n",
    "env = Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-fFj5sDiA0C"
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-370a2951be36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mshow_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gym-results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Record video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tried to pass invalid video frame, marking as broken: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.9.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "show_video(\"./gym-results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd2xLlapilZE"
   },
   "source": [
    "## REINFORCE\n",
    "\n",
    "**Q1: Implement the REINFORCE algorithm**\n",
    "\n",
    "The code is splitted in two parts:\n",
    "* The Model class defines the architecture of our neural network which takes as input the current state and returns the policy,\n",
    "* The Agent class is responsible for the training and evaluation procedure. You will need to code the method `optimize_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwsQ8NSPiCz7"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dim_observation, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.dim_observation = dim_observation\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dim_observation, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=self.n_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action = torch.multinomial(self.forward(state), 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrgtQMYbiwX7"
   },
   "source": [
    "Create the model based on the properties of the MDP you want to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6ieL_KJirq9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model we created correspond to:\n",
      "Model(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "env = gym.make(env_id)\n",
    "model = Model(env.observation_space.shape[0], env.action_space.n)\n",
    "print(f'The model we created correspond to:\\n{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwrVF9kti1e-"
   },
   "source": [
    "We provide a base agent that you will need to extend in the next cell with your implementation of `optimize_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWvKO66ii1zh"
   },
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.model = Model(self.env.observation_space.shape[0], self.env.action_space.n)\n",
    "        self.gamma = config['gamma']\n",
    "        \n",
    "        # the optimizer used by PyTorch (Stochastic Gradient, Adagrad, Adam, etc.)\n",
    "        self.optimizer = torch.optim.Adam(self.model.net.parameters(), lr=config['learning_rate'])\n",
    "        self.monitor_env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
    "    \n",
    "    # Method to implement\n",
    "    def _compute_returns(self, rewards):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            The array of rewards of one episode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards at each time step\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        for rewards=[1, 2, 3] this method outputs [1 + 2 * gamma + 3 * gamma**2, 2 + 3 * gamma, 3] \n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    # Method to implement\n",
    "    def optimize_model(self, n_trajectories):\n",
    "        \"\"\"Perform a gradient update using n_trajectories\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expectation card(D) in the formula above\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards of each trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, n_trajectories, n_update):\n",
    "        \"\"\"Training method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expected gradient\n",
    "        n_update : int\n",
    "            The number of gradient updates\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        rewards = []\n",
    "        for episode in range(n_update):\n",
    "            rewards.append(self.optimize_model(n_trajectories))\n",
    "            print(f'Episode {episode + 1}/{n_update}: rewards {round(rewards[-1].mean(), 2)} +/- {round(rewards[-1].std(), 2)}')\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards[i]) for i in range(len(rewards))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        \n",
    "    def evaluate(self, render=False):\n",
    "        \"\"\"Evaluate the agent on a single trajectory            \n",
    "        \"\"\"\n",
    "        \n",
    "        observation = self.monitor_env.reset()\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        reward_episode = 0\n",
    "        done = False\n",
    "            \n",
    "        while not done:\n",
    "            action = self.model.select_action(observation)\n",
    "            observation, reward, done, info = self.monitor_env.step(int(action))\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            reward_episode += reward\n",
    "        \n",
    "        self.monitor_env.close()\n",
    "        if render:\n",
    "            show_video(\"./gym-results\")\n",
    "            print(f'Reward: {reward_episode}')\n",
    "        print(f'Reward: {reward_episode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6mhw_A3i_8R"
   },
   "source": [
    "Finally you can implement your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbjP-7WHi9oc"
   },
   "outputs": [],
   "source": [
    "class REINFORCE(BaseAgent):\n",
    "    \n",
    "    def _compute_returns(self, rewards):\n",
    "                result = copy.deepcopy(rewards)\n",
    "        gamma = self.env.gamma\n",
    "        for i in range(len(rewards)):\n",
    "            curr_gamma = gamma\n",
    "            for j in range(i+1, len(rewards)):\n",
    "                result[i] += curr_gamma * rewards[j]\n",
    "                curr_gamma = curr_gamma * gamma\n",
    "        \n",
    "        return result\n",
    "    \n",
    "        \n",
    "    def optimize_model(self, n_trajectories):\n",
    "        ###\n",
    "        # Your code here\n",
    "        ###\n",
    "        \n",
    "        reward_trajectories = None\n",
    "        loss = None\n",
    "        \n",
    "        # The following lines take care of the gradient descent step for the variable loss\n",
    "        # that you need to compute.\n",
    "        \n",
    "        # Discard previous gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        return reward_trajectories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZAo0K8VjCYj"
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "learning_rate = 0.01\n",
    "gamma = 1 \n",
    "seed = 1235\n",
    "\n",
    "config = {\n",
    "    'env_id': env_id,\n",
    "    'learning_rate': learning_rate,\n",
    "    'seed': seed,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "print(\"Current config is:\")\n",
    "pprint(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4Ag7E3qjJas"
   },
   "outputs": [],
   "source": [
    "agent = REINFORCE(config)\n",
    "agent.train(n_trajectories=50, n_update=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3_dvBl8jUc3"
   },
   "source": [
    "Evaluate the agent over multiple episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPyqbIiCjO_-"
   },
   "outputs": [],
   "source": [
    "agent.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHM2_CRFkNoA"
   },
   "source": [
    "## Policy Evaluation as Supervised Learning\n",
    "\n",
    "**Q2: Implement batched gradient algorithm**\n",
    "\n",
    "Define network for Q-function (ValueNetwork) and policy (ActorNetwork)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9sxcz_eKF1M"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self(x).detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GKxKzs5khxM"
   },
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=-1)\n",
    "        return out\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        return torch.multinomial(self(x), 1).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBpNVkaAKLQz"
   },
   "source": [
    "Implement your (batched) gradient algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQ4Wu5B5KIIO"
   },
   "outputs": [],
   "source": [
    "class EvalAgent:\n",
    "\n",
    "    def __init__(self, config, policy):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.monitor_env = Monitor(self.env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
    "        self.gamma = config['gamma']\n",
    "        self.policy = policy\n",
    "\n",
    "        # Our network\n",
    "        self.value_network = ValueNetwork(self.env.observation_space.shape[0], 16, 1)\n",
    "\n",
    "        # optimizers\n",
    "        self.value_network_optimizer = optim.RMSprop(self.value_network.parameters(), \n",
    "                                                   lr=config['value_network']['learning_rate'])\n",
    "    \n",
    "    def _returns_advantages(self, rewards, dones, next_value):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            An array of shape (batch_size,) containing the rewards given by the env\n",
    "        dones : array\n",
    "            An array of shape (batch_size,) containing the done bool indicator given by the env\n",
    "        next_value : float\n",
    "            The value of the next state given by the value network\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        returns : array\n",
    "            The cumulative discounted rewards\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "    \n",
    "    \n",
    "    def optimize_model(self, observations, actions, returns, advantages):\n",
    "        \"\"\"Perform a gradient update using provided transitions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : array\n",
    "            The observations\n",
    "        actions : array\n",
    "            The actions\n",
    "        returns : array\n",
    "            The returns from each state\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss_value: int\n",
    "            The loss value\n",
    "        \"\"\"\n",
    "        actions = F.one_hot(torch.tensor(actions), self.env.action_space.n)\n",
    "        returns = torch.tensor(returns[:, None], dtype=torch.float)\n",
    "        observations = torch.tensor(observations, dtype=torch.float)\n",
    "        \n",
    "        # TODO\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def training_batch(self, epochs, batch_size):\n",
    "        \"\"\"Perform a training by batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : int\n",
    "            Number of epochs\n",
    "        batch_size : int\n",
    "            The size of a batch\n",
    "        \"\"\"\n",
    "        episode_count = 0\n",
    "        actions = np.empty((batch_size,), dtype=np.int)\n",
    "        dones = np.empty((batch_size,), dtype=np.bool)\n",
    "        rewards, values = np.empty((2, batch_size), dtype=np.float)\n",
    "        observations = np.empty((batch_size,) + self.env.observation_space.shape, dtype=np.float)\n",
    "        observation = self.env.reset()\n",
    "        mse_test = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Lets collect one batch\n",
    "            for i in range(batch_size):\n",
    "                observations[i] = #TODO\n",
    "                values[i] =  #TODO\n",
    "                actions[i] = #TODO\n",
    "\n",
    "                # step\n",
    "\n",
    "                if dones[i]:\n",
    "                    observation = self.env.reset()\n",
    "\n",
    "            # If our epiosde didn't end on the last step we need to compute the value for the last state\n",
    "            if dones[-1]:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = #TODO\n",
    "\n",
    "            # Update episode_count\n",
    "            episode_count += sum(dones)\n",
    "\n",
    "            # Compute returns\n",
    "            returns = self._returns_advantages(rewards, dones, next_value)\n",
    "\n",
    "            # Learning step !\n",
    "            self.optimize_model(observations, actions, returns)\n",
    "\n",
    "            # Test it every 50 epochs\n",
    "            if epoch % 25 == 0 or epoch == epochs - 1:\n",
    "                L = []\n",
    "                for _ in range(10):\n",
    "                    obs_states, y_mc = self.evaluate()\n",
    "                    y_hat = # compute prediction\n",
    "                    err = y_mc - y_hat\n",
    "                    mse = np.mean(err*2)\n",
    "                    L.append(mse.item())\n",
    "                mse_test.append(L)\n",
    "                print(f'Epoch {epoch}/{epochs}: MSE: {np.mean(mse)}')\n",
    "\n",
    "                observation = self.env.reset()\n",
    "\n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], mse_test[i]) for i in range(len(mse_test))))), columns=['Epoch', 'MSE'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"MSE\", data=r, ci='sd');\n",
    "\n",
    "        print(f'The trainnig was done over a total of {episode_count} episodes')\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "        \"\"\"Returns the observations and the estimated V-function (using first visit Monte-Carlo)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            An array of shape (batch_size,) containing the rewards given by the env\n",
    "        dones : array\n",
    "            An array of shape (batch_size,) containing the done bool indicator given by the env\n",
    "        values : array\n",
    "            An array of shape (batch_size,) containing the values given by the value network\n",
    "        next_value : float\n",
    "            The value of the next state given by the value network\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : array\n",
    "            Observations\n",
    "        returns : array\n",
    "            The estimate value function of each state\n",
    "        \"\"\"\n",
    "        env = self.monitor_env if render else self.env\n",
    "        observation = env.reset()\n",
    "        states = [observation.copy()]\n",
    "        rewards= []\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = # TODO\n",
    "            observation, reward, done, info = env.step(int(action))\n",
    "            for i in range(steps):\n",
    "                rewards[i] = rewards[i] + math.pow(self.gamma, steps-i)*reward\n",
    "            rewards.append(reward)\n",
    "            if not done:\n",
    "                states.append(observation.copy())\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            steps += 1\n",
    "\n",
    "        env.close()\n",
    "        if render:\n",
    "            show_video(\"./gym-results\")\n",
    "            print(f'Reward: {reward_episode}')\n",
    "        states = np.array(states).reshape(-1, self.env.observation_space.shape[0])\n",
    "        returns = np.array(rewards).reshape(-1,1)\n",
    "        return states, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IY1VIUHLNKn3"
   },
   "source": [
    "Define configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fvc-7jXDNNkG"
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "value_learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "seed = 1\n",
    "\n",
    "config_td = {\n",
    "    'env_id': env_id,\n",
    "    'gamma': gamma,\n",
    "    'seed': seed,\n",
    "    'value_network': {'learning_rate': value_learning_rate, 'reference': './CartPole_value.pt'}\n",
    "}\n",
    "\n",
    "print(\"Current config_td is:\")\n",
    "pprint(config_td)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2lRnaF3NPyy"
   },
   "source": [
    "Create policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkbHK_lXNShg"
   },
   "outputs": [],
   "source": [
    "env = gym.make(config_td['env_id'])\n",
    "policy = ActorNetwork(env.observation_space.shape[0], 16, env.action_space.n)\n",
    "policy.load_state_dict(torch.load('./mva_hands_on/data/CartPole_actor.pt'))\n",
    "state = torch.tensor(np.array(env.reset(), dtype=np.float32))\n",
    "print(\"pi(state) = \", policy.select_action(state))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y51M4EgtNU7Z"
   },
   "source": [
    "Run agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PFvnYLnNGvX"
   },
   "outputs": [],
   "source": [
    "agent = EvalAgent(config=config_td, policy=policy)\n",
    "agent.training_batch(epochs=1000, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03as0wkuja2A"
   },
   "source": [
    "## Advantage Actor Critic (A2C)\n",
    "**Q3: Implement the A2C method**\n",
    "\n",
    "As usual we provide a structure you can use as starting point.\n",
    "\n",
    "\n",
    "\n",
    "**Note:** try to reuse previous parts of previous code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4EmYpAsjhKh"
   },
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.monitor_env = Monitor(self.env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
    "        self.gamma = config['gamma']\n",
    "        \n",
    "        # Our two networks\n",
    "        self.value_network = ValueNetwork(self.env.observation_space.shape[0], 16, 1)\n",
    "        self.actor_network = ActorNetwork(self.env.observation_space.shape[0], 16, self.env.action_space.n)\n",
    "        \n",
    "        # Their optimizers\n",
    "        self.value_network_optimizer = optim.RMSprop(self.value_network.parameters(), lr=config['value_network']['learning_rate'])\n",
    "        self.actor_network_optimizer = optim.RMSprop(self.actor_network.parameters(), lr=config['actor_network']['learning_rate'])\n",
    "        \n",
    "    # Hint: use it during training_batch\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            An array of shape (batch_size,) containing the rewards given by the env\n",
    "        dones : array\n",
    "            An array of shape (batch_size,) containing the done bool indicator given by the env\n",
    "        values : array\n",
    "            An array of shape (batch_size,) containing the values given by the value network\n",
    "        next_value : float\n",
    "            The value of the next state given by the value network\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        returns : array\n",
    "            The cumulative discounted rewards\n",
    "        advantages : array\n",
    "            The advantages\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return NotImplementedError\n",
    "\n",
    "    def training_batch(self, epochs, batch_size):\n",
    "        \"\"\"Perform a training by batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : int\n",
    "            Number of epochs\n",
    "        batch_size : int\n",
    "            The size of a batch\n",
    "        \"\"\"\n",
    "        episode_count = 0\n",
    "        actions = np.empty((batch_size,), dtype=np.int)\n",
    "        dones = np.empty((batch_size,), dtype=np.bool)\n",
    "        rewards, values = np.empty((2, batch_size), dtype=np.float)\n",
    "        observations = np.empty((batch_size,) + self.env.observation_space.shape, dtype=np.float)\n",
    "        observation = self.env.reset()\n",
    "        rewards_test = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Lets collect one batch\n",
    "            \n",
    "            # If our epiosde didn't end on the last step we need to compute the value for the last state\n",
    "            \n",
    "            # Update episode_count\n",
    "            episode_count += sum(dones)\n",
    "\n",
    "            # Compute returns and advantages\n",
    "\n",
    "            # Learning step !\n",
    "\n",
    "            # Test it every 50 epochs\n",
    "            if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "                rewards_test.append(np.array([self.evaluate() for _ in range(50)]))\n",
    "                print(f'Epoch {epoch}/{epochs}: Mean rewards: {round(rewards_test[-1].mean(), 2)}, Std: {round(rewards_test[-1].std(), 2)}')\n",
    "\n",
    "                # Early stopping\n",
    "                if rewards_test[-1].mean() > 490 and epoch != epochs -1:\n",
    "                    print('Early stopping !')\n",
    "                    break\n",
    "                observation = self.env.reset()\n",
    "                    \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards_test[i]) for i in range(len(rewards_test))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        \n",
    "        print(f'The trainnig was done over a total of {episode_count} episodes')\n",
    "\n",
    "    def optimize_model(self, observations, actions, returns, advantages):\n",
    "        actions = F.one_hot(torch.tensor(actions), self.env.action_space.n)\n",
    "        returns = torch.tensor(returns[:, None], dtype=torch.float)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float)\n",
    "        observations = torch.tensor(observations, dtype=torch.float)\n",
    "\n",
    "        # MSE for the values\n",
    "        # Actor & Entropy loss\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "        env = self.monitor_env if render else self.env\n",
    "        observation = env.reset()\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        reward_episode = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            policy = self.actor_network(observation)\n",
    "            action = torch.multinomial(policy, 1)\n",
    "            observation, reward, done, info = env.step(int(action))\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            reward_episode += reward\n",
    "            \n",
    "        env.close()\n",
    "        if render:\n",
    "            show_video(\"./gym-results\")\n",
    "            print(f'Reward: {reward_episode}')\n",
    "        return reward_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEwrInsjkDH"
   },
   "source": [
    "Create configuration for A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwDDx78wjmG5"
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "value_learning_rate = 0.001\n",
    "actor_learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "entropy = 1\n",
    "seed = 1\n",
    "\n",
    "config_a2c = {\n",
    "    'env_id': env_id,\n",
    "    'gamma': gamma,\n",
    "    'seed': seed,\n",
    "    'value_network': {'learning_rate': value_learning_rate},\n",
    "    'actor_network': {'learning_rate': actor_learning_rate},\n",
    "    'entropy': entropy\n",
    "}\n",
    "\n",
    "print(\"Current config_a2c is:\")\n",
    "pprint(config_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xW7fe-8jvzY"
   },
   "source": [
    "Run the learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrZRJ7-yjryp"
   },
   "outputs": [],
   "source": [
    "agent = A2CAgent(config_a2c)\n",
    "rewards = agent.training_batch(1000, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzuNtPqTju64"
   },
   "source": [
    "Evaluate the agent over multiple episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw279M4Jj1y-"
   },
   "outputs": [],
   "source": [
    "agent.evaluate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MVARL19_part2.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
