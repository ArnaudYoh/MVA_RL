{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsaXz-ENaLbx"
   },
   "source": [
    "# Exploration in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHhiMrobaW8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mvarl_hands_on'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 39 (delta 14), reused 37 (delta 12), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf mvarl_hands_on/\n",
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git\n",
    "!cd mvarl_hands_on/ && git fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jpzHHRYd0pI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './mvarl_hands_on/utils')\n",
    "import os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gridworld import GridWorldWithPits\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQNxcIZtaSZJ"
   },
   "source": [
    "## Finite-Horizon Tabular MDPs\n",
    "We consider finite horizon problems with horizon $H$.For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
    "\n",
    "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
    "\n",
    "\n",
    "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
    "$$\n",
    "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
    "$$\n",
    "where $V_{H+1}(s) = 0$, for any $s$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Suggestion:\n",
    "- $V$ -> $(H+1, S)$-dimensional matrix\n",
    "- deterministic policy $\\pi$ -> $(H, S)$-dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Smh0opCibDY1"
   },
   "source": [
    "**Question 1:** implement backward induction for $V^\\pi$ and $V^\\star$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhLaiao3aR4N"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(P, R, H, policy):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            P: transition function (S,A,S)-dim matrix\n",
    "            R: reward function (S,A,S)-dim matrix\n",
    "            H: horizon\n",
    "            policy: a deterministic policy (H, S)-dim matrix\n",
    "            \n",
    "        Returns:\n",
    "            The V-function of the provided policy\n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    #TODO\n",
    "    return V\n",
    "\n",
    "def backward_induction(P, R, H):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            P: transition function (S,A,S)-dim matrix\n",
    "            R: reward function (S,A,S)-dim matrix\n",
    "            H: horizon\n",
    "            \n",
    "        Returns:\n",
    "            The optimal V-function\n",
    "            The optimal policy\n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    #TODO\n",
    "    return V, policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_l6lqhDbYmQ"
   },
   "source": [
    "Let's set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkOs-0y_bd61"
   },
   "outputs": [],
   "source": [
    "grid1 = [\n",
    "    ['', '', '', 'g'],\n",
    "    ['', 'x', '', ''],\n",
    "    ['s', '', '', '']\n",
    "]\n",
    "grid1_MAP = [\n",
    "    \"+-------+\",\n",
    "    \"| : : :G|\",\n",
    "    \"| :x: : |\",\n",
    "    \"|S: : : |\",\n",
    "    \"+-------+\",\n",
    "]\n",
    "\n",
    "\n",
    "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\n",
    "H = 6\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Paggla09bl6S"
   },
   "source": [
    "We should test previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgeanS2NbpQg"
   },
   "outputs": [],
   "source": [
    "V, optimal_pol = backward_induction(env.P, env.R, H)\n",
    "print(V)\n",
    "Vpi = evaluate_policy(env.P, env.R, H, optimal_pol)\n",
    "print(Vpi)\n",
    "assert np.allclose(V, Vpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MACThGeMb48-"
   },
   "source": [
    "Run the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbyoa9Qpb6Yb"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "for i in range(H):\n",
    "    next_state, reward, _, _ = env.step(optimal_pol[i, state])\n",
    "    env.render()\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZtdzpwUcHYj"
   },
   "source": [
    "Finally we are ready to implement our exploration algorithm.\n",
    "\n",
    "**Question 2**: implement the **UCB-VI** algorithm.\n",
    "\n",
    "UCBVI is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
    "In this setting, the regret is defined as\n",
    "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
    "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
    "\n",
    "The structure of the algorithm is as follow\n",
    "\n",
    "For $k = 1, \\ldots, K$ do<br>\n",
    "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
    "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
    ">> for $h=1, \\ldots, H$<br>\n",
    ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
    ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
    ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
    "\n",
    "<font color='#ed7d31'>Optimistic planning</font>\n",
    "\n",
    "UCBVI exploits exploration bonus to perform optimistic planning on the empirical MDP $(\\hat{p}, \\hat{r})$.\n",
    "The optimal Q-function of this MDP can be obtained using backward induction.\n",
    "\n",
    "The optimal Bellman operator for Q-function is defined similarly as\n",
    "$$\n",
    "Q_h^\\star(s,a) =  b(s,a) + \\sum_{s'} p(s'|s,a) \\left( r(s,a,s') + \\max_{a'} Q_{h+1}^\\star(s',a')\\right) \n",
    "$$\n",
    "where $Q_{H+1}(s,a) = 0$ and $b$ is an exploration bonus.\n",
    "\n",
    "<font color='#ed7d31'>Exploration Bonus</font>\n",
    "\n",
    "Using Hoeffding's bound we have that\n",
    "$$\n",
    "b_{k,h}(s,a) = 7(H-h+1)L\\sqrt{\\frac{1}{N_k(s,a)}}\n",
    "$$\n",
    "where $L = \\ln(5SAT/\\delta)$.\n",
    "\n",
    "A tighter exploration bonus is obtained using Bernstein's bound. Since it's expression is much more complicated, we provided the code (see `compute_bernstein_bonus`).\n",
    "\n",
    "\n",
    "Refer to [Minimax Regret Bounds for Reinforcement Learning](https://arxiv.org/abs/1703.05449) for additional details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXMJTmLPcOwS"
   },
   "outputs": [],
   "source": [
    "class UCBVI:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        np.random.seed(seed=config['seed'])\n",
    "        self.env = config['env']\n",
    "        self.horizon = config['horizon']\n",
    "        self.scale_factor = config['scale_factor']\n",
    "        self.nb_repetitions = config['repetitions']\n",
    "        self.nb_episodes = config['episodes']\n",
    "        assert config['b_type'] in ['hoeffding', 'bernstein']\n",
    "        self.b_type = config['b_type']\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        S, A = self.env.Ns, self.env.Na\n",
    "        self.delta = 0.1\n",
    "        self.t = 0\n",
    "        self.episode = 0\n",
    "        self.Phat = np.zeros((S, A, S))\n",
    "        self.Rhat = np.zeros((S, A, S))\n",
    "        self.N_sa = np.zeros((S, A), dtype=np.int)\n",
    "        self.N_sas = np.zeros((S, A, S), dtype=np.int)\n",
    "        self.policy = np.zeros((self.horizon, S), dtype=np.int)\n",
    "        self.V = np.zeros((self.horizon+1, S))\n",
    "        self.Q = np.zeros((self.horizon+1, S, A))\n",
    "        self.bonus = np.zeros((self.horizon, S, A))\n",
    "        \n",
    "    def get_optimistic_q(self):\n",
    "        \"\"\" Compute optimistic Q-function and associated greedy policy\n",
    "        \"\"\"\n",
    "        H = self.horizon\n",
    "        S, A = self.N_sa.shape\n",
    "        self.V.fill(0)\n",
    "        self.Q.fill(0)\n",
    "        for h in reversed(range(H)):\n",
    "            for s in range(S):\n",
    "                for a in range(A):\n",
    "                    # TODO\n",
    "                \n",
    "                self.V[h,s] = min(H-h+2., self.V[h,s])\n",
    "                \n",
    "    def compute_hoeffding_bonus(self):\n",
    "        \"\"\"Compute Hoeffding-based exploration bonus\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "    def compute_bernstein_bonus(self, h, Vhp1):\n",
    "        \"\"\"Compute Bernstein-based exploration bonus\n",
    "\n",
    "        Parameters:\n",
    "            h: state\n",
    "            Vhp1: value function at state h+1 (S-dim vector)\n",
    "        \"\"\"\n",
    "        S, A = self.N_sa.shape\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\n",
    "                n = max(1, self.N_sa[s][a])\n",
    "                var, mean = 0, 0\n",
    "                for i in range(S):\n",
    "                    mean += self.Phat[s,a,i] * Vhp1[i]\n",
    "                for i in range(S):\n",
    "                    var += self.Phat[s,a,i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\n",
    "                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon -h + 2) / (3*n)\n",
    "                T2 = np.sqrt(8 * (self.horizon -h + 2)**2  / n)\n",
    "                self.bonus[h,s,a] = self.scale_factor * (T1 + T2)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the internal statistics\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "    def run_episode(self):\n",
    "        episode_reward = 0\n",
    "        state = self.env.reset()\n",
    "        initial_state = state\n",
    "        self.get_optimistic_q()\n",
    "        \n",
    "        \n",
    "        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\n",
    "        self.episode_value.append(Vpi[0, initial_state])\n",
    "        \n",
    "        for h in range(self.horizon):\n",
    "            action = self.policy[h, state]\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            self.update(state, action, reward, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            self.t += 1\n",
    "        self.episode += 1\n",
    "        return initial_state, Vpi\n",
    "    \n",
    "    def train(self):\n",
    "        # compute true value function (for the regret)\n",
    "        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\n",
    "        regret = np.zeros((self.nb_repetitions, self.nb_episodes+1))\n",
    "        for rep in range(self.nb_repetitions):\n",
    "            self.reset()\n",
    "            old_regret = 0\n",
    "            for k in range(self.nb_episodes):\n",
    "                init_state, Vpi = self.run_episode()\n",
    "                episode_regret = #TODO compute the regret of this episode\n",
    "                old_regret += episode_regret\n",
    "                regret[rep, k+1] = old_regret\n",
    "        return regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MoSaZ3xOckmd"
   },
   "source": [
    "Define the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fjk1TbBock-G"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'env': env,\n",
    "    'scale_factor': 0.1, # we use a scaling factor in order to increase the convergence speed\n",
    "    'seed': 14,\n",
    "    'horizon': H,\n",
    "    'episodes': 4000,\n",
    "    'repetitions': 5,\n",
    "    'b_type': 'hoeffding' # [hoeffding, bernstein]\n",
    "}\n",
    "\n",
    "print(\"Current config is:\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSI7NR50cyFx"
   },
   "source": [
    "Run the agent and compare the behaviour with Hoeffding and Bernstein bound.\n",
    "\n",
    "A picture is automatically generated (it will show the regret average regret of the two algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8S3ObC9ncydR"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "regret = {}\n",
    "for bound in ['hoeffding', 'bernstein']:\n",
    "    tmp_config = copy.copy(config) \n",
    "    tmp_config['b_type'] = bound\n",
    "    agent = UCBVI(config=tmp_config)\n",
    "    regret[bound] = agent.train()\n",
    "\n",
    "    mean_regret = np.mean(regret[bound], axis=0)\n",
    "    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\n",
    "    x = np.arange(regret[bound].shape[1])\n",
    "    plt.plot(x, mean_regret, label=bound)\n",
    "    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MVARL19_part4.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
