{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsaXz-ENaLbx"
   },
   "source": [
    "# Exploration in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHhiMrobaW8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mvarl_hands_on'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 39 (delta 14), reused 37 (delta 12), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf mvarl_hands_on/\n",
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git\n",
    "!cd mvarl_hands_on/ && git fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jpzHHRYd0pI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './mvarl_hands_on/utils')\n",
    "import os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gridworld import GridWorldWithPits\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQNxcIZtaSZJ"
   },
   "source": [
    "## Finite-Horizon Tabular MDPs\n",
    "We consider finite horizon problems with horizon $H$.For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
    "\n",
    "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
    "\n",
    "\n",
    "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
    "$$\n",
    "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
    "$$\n",
    "where $V_{H+1}(s) = 0$, for any $s$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Suggestion:\n",
    "- $V$ -> $(H+1, S)$-dimensional matrix\n",
    "- deterministic policy $\\pi$ -> $(H, S)$-dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Smh0opCibDY1"
   },
   "source": [
    "**Question 1:** implement backward induction for $V^\\pi$ and $V^\\star$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhLaiao3aR4N"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(P, R, H, policy):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            P: transition function (S,A,S)-dim matrix\n",
    "            R: reward function (S,A,S)-dim matrix\n",
    "            H: horizon\n",
    "            policy: a deterministic policy (H, S)-dim matrix\n",
    "            \n",
    "        Returns:\n",
    "            The V-function of the provided policy\n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    V = np.zeros((H+1, S))\n",
    "    for h in range(H-1, -1, -1):\n",
    "        for s in range(S):\n",
    "            a = policy[h, s]\n",
    "            for s_next in range(S):\n",
    "                V[h, s] += P[s, a , s_next] * (R[s, a, s_next] + V[h+1, s_next])\n",
    "    \n",
    "    return V\n",
    "\n",
    "def backward_induction(P, R, H):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            P: transition function (S,A,S)-dim matrix\n",
    "            R: reward function (S,A,S)-dim matrix\n",
    "            H: horizon\n",
    "            \n",
    "        Returns:\n",
    "            The optimal V-function\n",
    "            The optimal policy\n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    \n",
    "    policy = np.full((H,S), -1)\n",
    "    V = np.zeros((H+1, S))\n",
    "    for h in reversed(range(H)):\n",
    "        for s in range(S):\n",
    "            expected_rewards = np.zeros(A)\n",
    "            for a in range(A):\n",
    "                expected = 0\n",
    "                for s_next in range(S):\n",
    "                    expected += P[s, a, s_next] * (R[s, a, s_next] + V[h+1, s_next])\n",
    "                expected_rewards[a] = expected\n",
    "            policy[h, s] = np.argmax(expected_rewards)\n",
    "            policy_action = policy[h, s] \n",
    "            for s_next in range(S):\n",
    "                V[h, s] += P[s, policy_action , s_next] * (R[s, policy_action, s_next] + V[h+1, s_next])\n",
    "                \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_l6lqhDbYmQ"
   },
   "source": [
    "Let's set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkOs-0y_bd61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|\u001b[43mS\u001b[0m: : : |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid1 = [\n",
    "    ['', '', '', 'g'],\n",
    "    ['', 'x', '', ''],\n",
    "    ['s', '', '', '']\n",
    "]\n",
    "grid1_MAP = [\n",
    "    \"+-------+\",\n",
    "    \"| : : :G|\",\n",
    "    \"| :x: : |\",\n",
    "    \"|S: : : |\",\n",
    "    \"+-------+\",\n",
    "]\n",
    "\n",
    "\n",
    "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\n",
    "H = 6\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Paggla09bl6S"
   },
   "source": [
    "We should test previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgeanS2NbpQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.98342174 3.98668316 3.99888469 3.99900565 3.93845103 3.38442821\n",
      "  3.98743386 3.99907706 3.91567934 3.95432206 3.99516932 3.99818051]\n",
      " [3.35518231 3.38566532 3.39841218 3.39933889 3.31201648 2.75064778\n",
      "  3.38668694 3.39869023 2.99900565 3.32685495 3.37162833 3.39692963]\n",
      " [2.73311111 2.762375   2.79823611 2.79966667 2.38934167 2.144975\n",
      "  2.76311944 2.79840833 2.39933889 2.38934167 2.74873056 2.77888611]\n",
      " [1.79966667 2.15083333 2.186      2.2        1.78983333 1.19\n",
      "  2.15733333 2.18616667 1.79966667 1.78983333 1.79966667 2.161     ]\n",
      " [1.2        1.19       1.58       1.6        1.19       0.6\n",
      "  1.19       1.58       1.2        1.19       1.2        1.2       ]\n",
      " [0.6        0.6        0.6        1.         0.6        0.\n",
      "  0.6        0.6        0.6        0.6        0.6        0.6       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[3.98342174 3.98668316 3.99888469 3.99900565 3.93845103 3.38442821\n",
      "  3.98743386 3.99907706 3.91567934 3.95432206 3.99516932 3.99818051]\n",
      " [3.35518231 3.38566532 3.39841218 3.39933889 3.31201648 2.75064778\n",
      "  3.38668694 3.39869023 2.99900565 3.32685495 3.37162833 3.39692963]\n",
      " [2.73311111 2.762375   2.79823611 2.79966667 2.38934167 2.144975\n",
      "  2.76311944 2.79840833 2.39933889 2.38934167 2.74873056 2.77888611]\n",
      " [1.79966667 2.15083333 2.186      2.2        1.78983333 1.19\n",
      "  2.15733333 2.18616667 1.79966667 1.78983333 1.79966667 2.161     ]\n",
      " [1.2        1.19       1.58       1.6        1.19       0.6\n",
      "  1.19       1.58       1.2        1.19       1.2        1.2       ]\n",
      " [0.6        0.6        0.6        1.         0.6        0.\n",
      "  0.6        0.6        0.6        0.6        0.6        0.6       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V_optimal, optimal_pol = backward_induction(env.P, env.R, H)\n",
    "print(V_optimal)\n",
    "Vpi = evaluate_policy(env.P, env.R, H, optimal_pol)\n",
    "print(Vpi)\n",
    "assert np.allclose(V_optimal, Vpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MACThGeMb48-"
   },
   "source": [
    "Run the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbyoa9Qpb6Yb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|\u001b[43mS\u001b[0m: : : |\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|S:\u001b[43m_\u001b[0m: : |\n",
      "+-------+\n",
      "  (right)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|S: :\u001b[43m_\u001b[0m: |\n",
      "+-------+\n",
      "  (right)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|S: : :\u001b[43m_\u001b[0m|\n",
      "+-------+\n",
      "  (right)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: :\u001b[43m_\u001b[0m|\n",
      "|S: : : |\n",
      "+-------+\n",
      "  (up)\n",
      "+-------+\n",
      "| : : :\u001b[42mG\u001b[0m|\n",
      "| :x: : |\n",
      "|S: : : |\n",
      "+-------+\n",
      "  (up)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|\u001b[43mS\u001b[0m: : : |\n",
      "+-------+\n",
      "  (right)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "for i in range(H):\n",
    "    next_state, reward, _, _ = env.step(optimal_pol[i, state])\n",
    "    env.render()\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZtdzpwUcHYj"
   },
   "source": [
    "Finally we are ready to implement our exploration algorithm.\n",
    "\n",
    "**Question 2**: implement the **UCB-VI** algorithm.\n",
    "\n",
    "UCBVI is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
    "In this setting, the regret is defined as\n",
    "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
    "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
    "\n",
    "The structure of the algorithm is as follow\n",
    "\n",
    "For $k = 1, \\ldots, K$ do<br>\n",
    "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
    "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
    ">> for $h=1, \\ldots, H$<br>\n",
    ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
    ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
    ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
    "\n",
    "<font color='#ed7d31'>Optimistic planning</font>\n",
    "\n",
    "UCBVI exploits exploration bonus to perform optimistic planning on the empirical MDP $(\\hat{p}, \\hat{r})$.\n",
    "The optimal Q-function of this MDP can be obtained using backward induction.\n",
    "\n",
    "The optimal Bellman operator for Q-function is defined similarly as\n",
    "$$\n",
    "Q_h^\\star(s,a) =  b(s,a) + \\sum_{s'} p(s'|s,a) \\left( r(s,a,s') + \\max_{a'} Q_{h+1}^\\star(s',a')\\right) \n",
    "$$\n",
    "where $Q_{H+1}(s,a) = 0$ and $b$ is an exploration bonus.\n",
    "\n",
    "<font color='#ed7d31'>Exploration Bonus</font>\n",
    "\n",
    "Using Hoeffding's bound we have that\n",
    "$$\n",
    "b_{k,h}(s,a) = 7(H-h+1)L\\sqrt{\\frac{1}{N_k(s,a)}}\n",
    "$$\n",
    "where $L = \\ln(5SAT/\\delta)$.\n",
    "\n",
    "A tighter exploration bonus is obtained using Bernstein's bound. Since it's expression is much more complicated, we provided the code (see `compute_bernstein_bonus`).\n",
    "\n",
    "\n",
    "Refer to [Minimax Regret Bounds for Reinforcement Learning](https://arxiv.org/abs/1703.05449) for additional details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXMJTmLPcOwS"
   },
   "outputs": [],
   "source": [
    "class UCBVI:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        np.random.seed(seed=config['seed'])\n",
    "        self.env = config['env']\n",
    "        self.horizon = config['horizon']\n",
    "        self.scale_factor = config['scale_factor']\n",
    "        self.nb_repetitions = config['repetitions']\n",
    "        self.nb_episodes = config['episodes']\n",
    "        assert config['b_type'] in ['hoeffding', 'bernstein']\n",
    "        self.b_type = config['b_type']\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        S, A = self.env.Ns, self.env.Na\n",
    "        self.delta = 0.1\n",
    "        self.t = 0\n",
    "        self.episode = 0\n",
    "        self.episode_value = list()\n",
    "        self.Phat = np.zeros((S, A, S))\n",
    "        self.Rhat = np.zeros((S, A, S))\n",
    "        self.N_sa = np.zeros((S, A), dtype=np.int)\n",
    "        self.N_sas = np.zeros((S, A, S), dtype=np.int)\n",
    "        self.policy = np.ones((self.horizon, S), dtype=np.int)\n",
    "        self.V = np.zeros((self.horizon+1, S))\n",
    "        self.Q = np.zeros((self.horizon+1, S, A))\n",
    "        self.bonus = np.zeros((self.horizon, S, A))\n",
    "        \n",
    "    def get_optimistic_q(self):\n",
    "        \"\"\" Compute optimistic Q-function and associated greedy policy\n",
    "        \"\"\"\n",
    "        H = self.horizon\n",
    "        S, A = self.N_sa.shape\n",
    "        self.V.fill(0)\n",
    "        self.Q.fill(0)\n",
    "        for h in reversed(range(H)):\n",
    "            if self.b_type == 'hoeffing':\n",
    "                self.compute_hoeffding_bonus(h)\n",
    "            if self.b_type == 'bernstein':\n",
    "                self.compute_bernstein_bonus(h, self.V[h+1])\n",
    "            for s in range(S):\n",
    "                expected_rewards = np.zeros(A)\n",
    "                for a in range(A):  \n",
    "                    if self.N_sa[s][a] > 0:\n",
    "                        expected = 0\n",
    "                        for s_next in range(S):\n",
    "                            expected += self.Phat[s,a,s_next] * (self.Rhat[s,a,s_next] + \n",
    "                                                                 np.max(self.Q[h+1,s_next]))\n",
    "                        expected_rewards[a] = expected\n",
    "                        self.Q[h,s,a] = self.bonus[h,s,a] + expected\n",
    "                    else:\n",
    "                        self.Q[h,s,a] = H\n",
    "                self.policy[h, s] = np.argmax(self.Q[h,s])\n",
    "                self.V[h,s] = min(H-h+2., self.V[h,s])   \n",
    "        #TODO\n",
    "                \n",
    "    def compute_hoeffding_bonus(self, h):\n",
    "        \"\"\"Compute Hoeffding-based exploration bonus\n",
    "        \"\"\"\n",
    "        S, A = self.N_sa.shape\n",
    "        H = self.horizon\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\n",
    "                n = np.sqrt(1 / max(self.N_sa[s][a], 1))\n",
    "                self.bonus[h,s,a] =  7(H - h + 1) * L * n\n",
    "        # TODO\n",
    "        \n",
    "    def compute_bernstein_bonus(self, h, Vhp1):\n",
    "        \"\"\"Compute Bernstein-based exploration bonus\n",
    "\n",
    "        Parameters:\n",
    "            h: state\n",
    "            Vhp1: value function at state h+1 (S-dim vector)\n",
    "        \"\"\"\n",
    "        S, A = self.N_sa.shape\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\n",
    "                n = max(1, self.N_sa[s][a])\n",
    "                var, mean = 0, 0\n",
    "                for i in range(S):\n",
    "                    mean += self.Phat[s,a,i] * Vhp1[i]\n",
    "                for i in range(S):\n",
    "                    var += self.Phat[s,a,i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\n",
    "                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon -h + 2) / (3*n)\n",
    "                T2 = np.sqrt(8 * (self.horizon -h + 2)**2  / n)\n",
    "                self.bonus[h,s,a] = self.scale_factor * (T1 + T2)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the internal statistics\n",
    "        \"\"\"\n",
    "        self.N_sas[state, action, next_state] += 1\n",
    "        self.N_sa[state, action] += 1\n",
    "        self.Phat[state, action, next_state] = self.N_sas[state, action, next_state] / self.N_sa[state, action]\n",
    "        self.Rhat[state, action, next_state] = ((self.N_sas[state, action, next_state] -1) * self.Rhat[state, action, next_state] + reward) / self.N_sas[state, action, next_state]\n",
    "        \n",
    "        \n",
    "    def run_episode(self):\n",
    "        episode_reward = 0\n",
    "        state = self.env.reset()\n",
    "        initial_state = state\n",
    "        self.get_optimistic_q()\n",
    "        \n",
    "        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\n",
    "        self.episode_value.append(Vpi[0, initial_state])\n",
    "        \n",
    "        for h in range(self.horizon):\n",
    "            action = self.policy[h, state]\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            self.update(state, action, reward, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            self.t += 1\n",
    "        self.episode += 1\n",
    "        return initial_state, Vpi\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # compute true value function (for the regret)\n",
    "        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\n",
    "        regret = np.zeros((self.nb_repetitions, self.nb_episodes+1))\n",
    "        for rep in range(self.nb_repetitions):\n",
    "            self.reset()\n",
    "            old_regret = 0\n",
    "            for k in range(self.nb_episodes):\n",
    "                init_state, Vpi = self.run_episode()\n",
    "                episode_regret = V_optimal[0, init_state] - Vpi[0, init_state]\n",
    "                old_regret += episode_regret\n",
    "                regret[rep, k+1] = old_regret\n",
    "        return regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MoSaZ3xOckmd"
   },
   "source": [
    "Define the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fjk1TbBock-G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config is:\n",
      "{'b_type': 'hoeffding',\n",
      " 'env': <gridworld.GridWorldWithPits object at 0x7fd623652790>,\n",
      " 'episodes': 2000,\n",
      " 'horizon': 6,\n",
      " 'repetitions': 5,\n",
      " 'scale_factor': 0.1,\n",
      " 'seed': 14}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'env': env,\n",
    "    'scale_factor': 0.1, # we use a scaling factor in order to increase the convergence speed\n",
    "    'seed': 14,\n",
    "    'horizon': H,\n",
    "    #'episodes': 4000,\n",
    "    'episodes': 2000,\n",
    "    'repetitions': 5,\n",
    "    'b_type': 'hoeffding' # [hoeffding, bernstein]\n",
    "}\n",
    "\n",
    "print(\"Current config is:\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSI7NR50cyFx"
   },
   "source": [
    "Run the agent and compare the behaviour with Hoeffding and Bernstein bound.\n",
    "\n",
    "A picture is automatically generated (it will show the regret average regret of the two algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8S3ObC9ncydR"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "regret = {}\n",
    "for bound in ['hoeffding', 'bernstein']:\n",
    "    tmp_config = copy.copy(config) \n",
    "    tmp_config['b_type'] = bound\n",
    "    agent = UCBVI(config=tmp_config)\n",
    "    regret[bound] = agent.train()\n",
    "\n",
    "    mean_regret = np.mean(regret[bound], axis=0)\n",
    "    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\n",
    "    x = np.arange(regret[bound].shape[1])\n",
    "    plt.plot(x, mean_regret, label=bound)\n",
    "    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MVARL19_part4.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
